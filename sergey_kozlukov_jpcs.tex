\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\usepackage{iopams}
% amsmath defined substack
\usepackage{amsmath}

\usepackage{todonotes}

\begin{document}
\title{The method of similar operators in the study of graph spectra}

\author{Sergey V Kozlukov}

\address{\emph{Voronezh State University}, 1 Universitetskaya Ploshad', Voronezh, RU 394036}

\ead{ithesaboteur@yandex.com}

\begin{abstract}
    The~method~of~similar~operators~\cite{baskakov1983methods,baskakov2014memory,baskakov2017method,baskakov2013completeness}
        is used to investigate spectral properties
        of a certain class of matrices in the context of graphs~\cite{van2003graphs,cvetkovic1980spectra}.
    Specifically, we consider matrices
        obtained as combinations
        of kronecker products~\cite{bellman-matrices-kron}
        and small-norm perturbations.
    The estimates of the spectrum and the eigenvectors
        of such matrices are given.
\end{abstract}

\section{Introduction}
Consider a~directed graph on \( N \) vertices.
Its adjacency matrix is defined as
    a matrix
    \( A = (a_{ij}) \)
    of the size \( N\times N \),
    in which \( a_{ij} \)
    is the number of edges
    from the vertex \( i \)
    to the vertex \( j \).
One can also consider generalized
    adjacency matrices~\cite{van2003graphs}
    which are defined as
    arbitrary linear combinations
    of matrices \( A \) (or its transpose \( A^\mathtt{T} \)),
    \( E \) (an identity matrix)
    and \( J \) (an all-ones matrix).
These matrices arise naturally
    in some stochastic models.
Spectral  properties of these matrices
    often play a vital role in such models.
For instance, the markovian random walk on a~graph
    yields the notion of eigenvector centrality
    in a~network~\cite{bonacich1972factoring}.
The transition matrix of such a random walk
    is generalized adjacency matrix of a graph.
The score of the vertex \( i \)
    is defined as the \( i \)'th coordinate
    of the dominating left eigenvector
    of the transition matrix.
The largest left eigenvalue of an irreducible column-stochastic matrix
    is \( 1 \) and the dominating left eigenvector
    defines the only stationary distribution
    of a random walk.
The PageRank~\cite{ilprints422} algorithm
    originally used by Google
    to compute the eigenvector centrality
    relies on the Power-Method.
Its speed of convergence depends on
    the ratio of the two largest absolute eigenvalues.
Stability of the stationary distribution
    is determined~\cite{meyer1994sensitivity}
    by the condition number
    which is bounded from below
    by a spectral gap --- the distance between
    the two largest eigenvalues
    of the transition matrix.
The method of estimation of almost-invariant sets
    proposed in~\cite{schwartz2006fluctuation}
    relies as well on spectral decompositions of such matrices.
In the Susceptible-Infective-Susceptible model
    a viral spread in a network
    is modeled as a markov process
    with \( 2^N \) states.
An asymptotic (endemic or epidemic) behaviour of this system
    is determined by the spectral radius (the absolute largest eigenvalue)
    of the adjacency matrix
    and the rates of curage and infection.
For more details and comprehensive description
    of the graph spectra theory
    refer to~\cite{cvetkovic1980spectra,godsil2013algebraic}.

\section{Methods and materials}

We use the abstract method of similar operators
    to derive estimates of eigenvalues and eigenvectors
    of certain types of such matrices.
This method originates from Friedrichs~\cite{friedrichs1965advanced}
    and was later developed in abstract setting
    by Baskakov~\cite{baskakov1983methods,baskakov2014memory,baskakov2017method,baskakov2013completeness}.
Here we will only state the required notation and theorems
    in a simplified form that accounts for finite-dimensionality
    of the problem.

Let \( \mathbb{K}\in \{ \mathbb{R}, \mathbb{C} \} \)
    be a field of either real or complex numbers.
We consider the vector space \( \mathbb{K}^n,\ n\in \mathbb{N} \)
    to be supplied with euclidian structure:
    \[
        (x, y){=}\sum_k x_k\overline{y_k},
        \ x{=}(x_1,\ldots, x_n),
        \ y=(y_1,\ldots, y_n)
        \in \mathbb{K}^n
        \]
    and the \( \mathrm{L}_2 \) norm:
    \(
        \|x\|_2^2{=}(x,x).
        \)
When \( V_1, V_2 \) are normed vector spaces
    we denote by \( L(V_1, V_2) \)
    the space of bounded linear mappings
    from \( V_1 \) to \( V_2 \).
An algebra of bounded linear endomorphisms
    from a Banach space \( V \)
    into itself
    is denoted by \( L(V) = L(V, V) \).
It is a Banach algebra with the operator norm:
    \[
        \|A\|_{\mathrm{op}} =
        \sup_{\substack{\|x\|=1,\\ x\in V}} \|A x\|,\ A\in L(V).
        \]
Together with \( L(\mathbb{K}^n, \mathbb{K}^m) \)
    we consider its isomorphic space \( \mathbb{K}^{m{\times}n} \)
    of matrices of the size \( m{\times}n \)
    with entries from a field \( \mathbb{K} \).
The space \( \mathbb{K}^{n{\times}n}\sim L(\mathbb{K}^n) \)
    forms a Banach algebra
    when supplied with a submultiplicative norm
    \( \|\cdot\| \),
    e.g.: \( \|A\|_{\mathrm{op}} = \sup_{\|x\|_2=1,\ x\in \mathbb{K}^n} \|A x\|_2,\ \)
    \( \|A\|_{\mathrm{F}} = \sqrt{\sum_{i,j} |a_{ij}|^2},\ \)
    for a matrix 
    \( A{=}(a_{ij})\in\mathbb{K}^{n\times n} \).
Finally we are going to use the isomorphic spaces
    \( L(L(\mathbb{K}^n)) \) and \( L(\mathbb{K}^{n{\times}n}) \)
    with the operator norm.
We will follow {Krein \todo{cite smth}}
    and refer to elements of \( L(\mathbb{K}^{n{\times}n}) \)
    as ``transformers''.

The spectrum of a matrix \( A \)
    (the set of its eigenvalues)
    will be denoted as \( \sigma(A) \).
We call two matrices \( A_1, A_2 \) \emph{similar}
    if there is an invertible matrix \( U \)
    (called similarity matrix)
    such that \( A_1 U = U A_2 \).
Similar matrices share some spectral properties:
    they are isospectral (\( \sigma(A_1) = \sigma(A_2) \))
    and \( U \) maps eigenvectors of one to another's:
    \( A_2 x = \lambda x \implies A_1 U x = \lambda U x \).

The most important notion
    of the abstract method of similar operators
    is that of an \emph{admissible triple}.
For our specific purposes it suffices to say
    that \( (\mathbb{K}^n, J, \Gamma) \)
    forms an \emph{admissible triple}
    for a matrix \( A\in\mathbb{K}^{n{\times}n} \)
    if the following conditions are met:
\begin{itemize}
    \item \( J, \Gamma \in L(\mathbb{K}^{n{\times}n}) \)
        are transformers;
    \item \( J \) is a projection (\( J^2 = J \));
    \item  \( \Gamma \) is defined by equations:
        \[
            A \Gamma X - (\Gamma X) A = X - JX,
        \]
        \[
            J\Gamma X = 0,\ X\in\mathbb{K}^{n{\times}n}.
        \]
\end{itemize}

Then the main theorem of the method
    may be formulated as follows:

\textbf{Theorem.}
{\it
    Consider a matrix \( A - B \)
        with \( A, B \in \mathbb{K}^{n{\times}n} \).
    Suppose \( (\mathbb{K}^{n{\times}n}, J, \Gamma) \)
        is an admissible triple for a matrix \( A \)
        and the following inequality holds:
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]

    Then there exists such a matrix \( X^o\in\mathbb{K}^{n{\times}n} \)
        that
        \( A - B \) is similar to a matrix \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|.
        \]
    Such \( X^o \) can be found as the limit of a convergent sequence
        \( \{ \Phi^k(0);\ k\in\mathbb{N} \} \)
        in a Banach algebra \( \mathbb{K}^{n{\times}n} \),
        where \( \Phi \) is a nonlinear mapping given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B
    \]
        and \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes composition.
}

\section*{References}
\bibliographystyle{iopart-num}
\bibliography{sergey_kozlukov_jpcs}{}
\end{document}
