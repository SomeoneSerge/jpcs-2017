\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\usepackage{iopams}
% amsmath defined substack
\usepackage{amsmath}

\usepackage{todonotes}

\begin{document}
\title{The method of similar operators in the study of graph spectra}

\author{Sergey V Kozlukov}

\address{\emph{Voronezh State University}, 1 Universitetskaya Ploshad', Voronezh, RU 394036}

\ead{ithesaboteur@yandex.com}

\begin{abstract}
    The~method~of~similar~operators~\cite{baskakov1983methods,baskakov2014memory,baskakov2017method,baskakov2013completeness}
        is used to investigate spectral properties
        of a certain class of matrices in context of graphs~\cite{van2003graphs,cvetkovic1980spectra}.
    Specifically, we consider matrices
        obtained as combinations
        of kronecker products~\cite{bellman-matrices-kron,XIANG2005210}
        and small-norm perturbations.
    The estimates of the spectrum and the eigenvectors
        of such matrices are given.
\end{abstract}

\section{Introduction}
Consider a~directed graph on \( N \) vertices.
Its adjacency matrix is defined as
    a matrix
    \( A = (a_{ij}) \)
    of the size \( N\times N \),
    in which \( a_{ij} \)
    is the number of edges
    from the vertex \( i \)
    to the vertex \( j \).
One can also consider generalized
    adjacency matrices~\cite{van2003graphs}
    which are defined as
    arbitrary linear combinations
    of matrices \( A \) (or its transpose \( A^\mathtt{T} \)),
    \( E \) (the identity matrix)
    and \( J_N \) (the all-ones matrix).
These matrices arise naturally
    in some stochastic models.
Spectral  properties of these matrices
    often play a vital role in such models.
For instance, the markovian random walk on a~graph
    yields the notion of eigenvector centrality
    in a~network~\cite{bonacich1972factoring}.
The transition matrix of such a random walk
    is generalized adjacency matrix of a graph.
The score of the vertex \( i \)
    is defined as the \( i \)'th coordinate
    of the dominating left eigenvector
    of the transition matrix.
The largest left eigenvalue of an irreducible column-stochastic matrix
    is \( 1 \) and the dominating left eigenvector
    defines the only stationary distribution
    of a random walk.
The PageRank~\cite{ilprints422} algorithm
    originally used by Google
    to compute the eigenvector centrality
    relies on the Power-Method.
Its speed of convergence depends on
    the ratio of the two largest absolute eigenvalues.
Stability of the stationary distribution
    is determined~\cite{meyer1994sensitivity}
    by the condition number
    which is bounded from below
    by a spectral gap --- the distance between
    the two largest eigenvalues
    of the transition matrix.
The method of estimation of almost-invariant sets
    proposed in~\cite{schwartz2006fluctuation}
    relies as well on spectral decompositions of such matrices.
In the Susceptible-Infective-Susceptible model
    a viral spread in a network
    is modeled as a markov process
    with \( 2^N \) states.
An asymptotic (endemic or epidemic) behaviour of this system
    is determined by a spectral radius (the largest absolute eigenvalue)
    of the adjacency matrix
    and the rates of curage and infection.
For more details and comprehensive description
    of the graph spectra theory
    refer to~\cite{cvetkovic1980spectra,godsil2013algebraic}.

\section{Methods and materials}

We use the abstract method of similar operators
    to derive estimates of eigenvalues and eigenvectors
    of certain types of such matrices.
This method originates from Friedrichs~\cite{friedrichs1965advanced}
    and was later developed in abstract setting
    by Baskakov~\cite{baskakov1983methods,baskakov2014memory,baskakov2017method,baskakov2013completeness}.
Here we will only state the required notation and theorems
    in a simplified form that accounts for finite-dimensionality
    of the problem.

Let \( \mathbb{K}\in \{ \mathbb{R}, \mathbb{C} \} \)
    be a field of either real or complex numbers.
We consider the vector space \( \mathbb{K}^n,\ n\in \mathbb{N} \)
    to be supplied with Euclidean structure:
    \[
        (x, y){=}\sum_{k=1}^n x_k\overline{y_k},
        \ x{=}(x_1,\ldots, x_n),
        \ y=(y_1,\ldots, y_n)
        \in \mathbb{K}^n
        \]
    and the \( \mathrm{L}_2 \) norm:
    \(
        \|x\|_2^2{=}(x,x).
        \)
We also consider a canonical basis \( e_1, \ldots, e_n \)
    in \( \mathbb{K}^n \) given by
    \( {(e_i)}_j = \delta_{ij},\ i,j=\overline{1,n} \)
    (\(\delta_{ij} \) is the Kronecker symbol).
When \( V_1, V_2 \) are normed vector spaces
    we denote by \( L(V_1, V_2) \)
    the space of bounded linear mappings
    from \( V_1 \) to \( V_2 \).
An algebra of bounded linear endomorphisms
    from a Banach space \( V \)
    into itself
    is denoted by \( L(V) = L(V, V) \).
It is a Banach algebra with the operator norm:
    \[
        \|A\|_{\mathrm{op}} =
        \sup_{\substack{\|x\|=1,\\ x\in V}} \|A x\|,\ A\in L(V).
        \]
Together with \( L(\mathbb{K}^n, \mathbb{K}^m) \)
    we consider its isomorphic space \( \mathbb{K}^{m{\times}n} \)
    of matrices of the size \( m{\times}n \)
    with entries from a field \( \mathbb{K} \).
The space \( \mathbb{K}^{n{\times}n}\sim L(\mathbb{K}^n) \)
    forms a Banach algebra
    when supplied with a submultiplicative norm
    \( \|\cdot\| \),
    e.g.: \( \|A\|_{\mathrm{op}} = \sup_{\|x\|_2=1,\ x\in \mathbb{K}^n} \|A x\|_2,\ \)
    \( \|A\|_{\mathrm{F}} = \sqrt{\sum_{i,j} |a_{ij}|^2},\ \)
    for a matrix 
    \( A{=}(a_{ij})\in\mathbb{K}^{n\times n} \).
Finally we are going to use the isomorphic spaces
    \( L(L(\mathbb{K}^n)) \) and \( L(\mathbb{K}^{n{\times}n}) \)
    with the operator norm.
We will follow {Krein \todo{cite smth}}
    and refer to elements of \( L(\mathbb{K}^{n{\times}n}) \)
    as ``transformers''.

The spectrum of a matrix \( A \)
    (the set of its eigenvalues)
    will be denoted as \( \sigma(A) \).
We call two matrices \( A_1, A_2 \) \emph{similar}
    if there is an invertible matrix \( U \)
    (called similarity matrix)
    such that \( A_1 U = U A_2 \).
Similar matrices share some spectral properties:
    they are isospectral (\( \sigma(A_1) = \sigma(A_2) \))
    and \( U \) maps eigenvectors of one to another's:
    \( A_2 x = \lambda x \implies A_1 U x = \lambda U x \).

The most important notion
    of the abstract method of similar operators
    is that of an \emph{admissible triple}.
For our specific purposes it suffices to say
    that \( (\mathbb{K}^n, J, \Gamma) \)
    forms an \emph{admissible triple}
    for a matrix \( A\in\mathbb{K}^{n{\times}n} \)
    if the following conditions are met:
\begin{itemize}
    \item \( J, \Gamma \in L(\mathbb{K}^{n{\times}n}) \)
        are transformers;
    \item \( J \) is a projection (\( J^2 = J \));
    \item  \( \Gamma \) satisfies the equations:
        \[
            A \Gamma X - (\Gamma X) A = X - JX,
        \]
        \[
            J\Gamma X = 0,\ X\in\mathbb{K}^{n{\times}n}.
        \]
\end{itemize}

The main theorem of the method
    may be then formulated as follows:

\textbf{Theorem.}
{\it
    Consider a matrix \( A - B \)
        with \( A, B \in \mathbb{K}^{n{\times}n} \).
    Suppose \( (\mathbb{K}^{n{\times}n}, J, \Gamma) \)
        is an admissible triple for a matrix \( A \)
        and the following inequality holds:
        \[
            \|B\|\|\Gamma\|_{\mathrm{op}} \leq \frac14.
        \]

    Then there exists such a matrix \( X^o\in\mathbb{K}^{n{\times}n} \)
        that
        \( A - B \) is similar to a matrix \( A - J X^o \);
        the similarity matrix is \( E + \Gamma X^o \);
        the following estimates are valid:
        \[
            \|X^o - B\| \leq 3 \left\|B\right\|,
        \]
        \[
            \operatorname{spr}(X^o) \leq \|X^o\| \leq 4 \left\|B\right\|,
        \]
        where \( \operatorname{spr}(X^o) \)
        is the spectral radius of \( X^o \) (the largest absolute eigenvalue).
    Such \( X^o \) can be found as the limit of a convergent sequence
        \( \left( \Phi^k(0);\ k\in\mathbb{N} \right) \)
        in a Banach algebra \( \mathbb{K}^{n{\times}n} \).
        Here \( \Phi \) is a nonlinear mapping given by
    \[
        \Phi(X) = B\Gamma X - (\Gamma X)J(B + B\Gamma X) + B
    \]
        and \( \Phi^k = \underbrace{\Phi\circ\cdots\circ\Phi}_{k\ \text{copies}} \)
        denotes composition.
}

\section{Results and discussion}

\subsection*{Almost-complete graph example.}

Consider a digraph defined by the following adjacency matrix:
\[
    A = J_N - B = \begin{pmatrix}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{pmatrix} - B,
\]

Here \( J_N \) is the all-ones matrix.
The unity on the intersection
of \( i \)'th row and \( j \)'th column of \( B \)
corresponds to an edge from \( i \) to \( j \)
being absent in the graph.

One can easily find the minimal annihilating polynomial of \( J_N \)
    to be \( \lambda(\lambda - N) \).
This comes from the fact that \( J_N^2 = N J_N \).
Consequently the spectrum of \( J_N \) is
\[
    \sigma(J_N) = \left\{0, N\right\}.
\]
The only non-zero eigenvalue \( N \) has a corresponding eigenvector
\[
    h_N = \frac{1}{\sqrt{N}} \left(1, \ldots, 1\right)\in\mathbb{R}^N.
\]
The null-space of \( J_N \) is orthogonal to \( h_N \)
    and has an orthonormal basis of vectors:
\[
    h_k = \frac{1}{\sqrt{k(k+1)}} \left(\underbrace{1, \ldots, 1}_{k\ \text{copies}}, -k, 0, \ldots, 0\right),\ k=\overline{1, N-1}.
\]

We conclude that the adjacency matrix \( A = J_N - B \) of this graph
    is similar to a matrix \( \mathcal{A} - \mathcal{B} \)
    where \( A \) is a block matrix (in what follows throughout this subsection block sizes are the same and will be omitted):
    \[
        \mathcal{A} = \left(\begin{array}{c|c}
        N & \mathbf{0}_{1{\times}(N{-}1)} \\ \hline
            \mathbf{0}_{(N{-}1){\times}1} & \mathbf{0}_{(N{-}1){\times}(N{-}1)}
        \end{array}\right) \in \mathbb{R}^{N{\times}N}
    \]
    and \( \mathcal{B} \) is obtained with a similarity transform:
    \[
        \mathcal{B} = U^{-1} B U \in \mathbb{R}^{N{\times}N}.
    \]
The similarity matrix \( U \) is given by stacking column-eigenvectors:
    \[
        U = \operatorname{columns}(h_N, h_1, \ldots, h_{N-1}).
    \]

Following the general method, we shall construct an admissible triple.
Since \( \mathcal{A} \) is block-diagonal
    it is natural to set \( J \) with the following formula:
    \[
        JX =
        \left(\begin{array}{c|c}
            x_{11} & \mathbf{0} \\ \hline
            \mathbf{0} & X_{22}
        \end{array}\right)
    \]
    for all block matrices
    \[
        X =
        \left(\begin{array}{c|c}
            x_{11} & X_{12} \\ \hline
            X_{21} & X_{22}
        \end{array}\right)\in\mathbb{K}^{N{\times}N}.
    \]
Then \( A - JX \) is block-diagonal for any \( X \)
    and its spectrum is the union
    of its diagonal blocks' spectra:
    \( \sigma(A - JX) = \{N, \sigma(-X_{22}) \} \).

Now we are able to find the corresponding transformer \( \Gamma \).
Suppose it is defined by a formula
    \[ \Gamma X = \begin{pmatrix}
        \Gamma_{11}(X) & \Gamma_{12}(X) \\
        \Gamma_{21}(X) & \Gamma_{22}(X)
        \end{pmatrix}.
    \]
Then the equations
    \[
        \mathcal{A} \Gamma X - (\Gamma X)\mathcal{A} =
        N
        \begin{pmatrix}
          0 & \Gamma_{12}(X) \\
          -\Gamma_{21}(X) & 0
        \end{pmatrix} = X - JX,
     \]
and \( J\Gamma X = 0 \) yield the result
    \[
        \Gamma X = \frac{1}{N} \begin{pmatrix} 0 & X_{12} \\ -X_{21} & 0 \end{pmatrix},\ X\in\mathbb{K}^{N{\times}N}.
    \]

The last step is to estimate the norms and apply the theorem.
One can verify that \( \|\Gamma\|_{\mathrm{op}} = \frac1N \).
It is also apparent that \( \|\mathcal{B}\|_{\mathrm{op}} = \|B\|_{\mathrm{op}} \leq \|B\|_{mathrm{F}} \)
    since multiplication by orthogonal matrix\( U \) is an isometry in Euclidean space.
The Frobenius norm \( {\|B\|_{\mathrm{F}} = \sqrt{\sum_{ij} b_ij^2} = M} \)
    of \( B \)
    reduces to the square root of the number of absent edges.
A direct implication is the following

\textbf{Theorem.}
{\it
    Suppose the number of absent edges is
    \[ M < \frac{1}{16} N^2. \]
    Then the spectrum of the adjacency matrix \( A = J_N - B \)
        can be represented as disjoint union
    \[
        \sigma(A) = \{ N - x_{11}^o \} \cup \sigma_2.
    \]
    The dominating eigenvector of \( A \) is
    \[
        \hat{h}_N = U(E+\Gamma X^o) e_1 =
            h_N - \frac1N (X_{21,(1)}^o h_1 + \cdots + X_{21, (N{-}1)}^o h_{N{-}1}),
    \]
    where \( X_{21,(i)}^o,\ i=\overline{1,N{-}1} \) are the coordinates
    of the vector \( X_{21}^o \).
    Moreover \( \hat{h}_N\in\mathbb{R}^{N} \),
    \( x_{11}^o\in\mathbb{R} \) and \( \sigma_2\subset\mathbb{C} \)
    satisfy the following inequalities:
    \[
        \|\hat{h}_N - h_N\|_2 \leq 4\frac{\sqrt{M}}{N},
    \]
    \[
        \lvert x_{11}^o \rvert,
        \ \sup_{\lambda\in\sigma_2} \lvert\lambda\rvert \leq 4\sqrt{M}.
    \]
}

\subsection*{A-tiled matrix example.}

Now let \( A\in\mathbb{K}^{m{\times}m} \)
    and consider the following (``A-tiled'') block-matrix
    \[
        \mathbb{A} =
        \begin{pmatrix}
            A & \cdots & A \\
            \vdots & \ddots & \vdots \\
            A & \cdots & A
        \end{pmatrix}
        \in\mathbb{K}^{{mn}{\times}{mn}}
    \]
    and a perturbed matrix
    \[
        \mathbb{A} - \mathbb{B},\ \mathbb{B}\in\mathbb{K}^{{mn}{\times}{mn}}.
    \]

\textbf{Lemma.}
{\it
    Suppose \( A \) is an invertible self-adjoint matrix.
    Then it has \( m \) orthonormal eigenvectors \( h_1, \ldots, h_m \)
    (\(\left\|h_i\right\|_2 = 1,\ i{=}\overline{1,m}\))
    with the corresponding eigenvalues
    \( \lambda_1, \ldots, \lambda_m \neq 0\).
    The spectrum of \( \mathbb{A} \) is
    \[
        \sigma(\mathbb{A}) = \{0\}\cup n\sigma(A) = \{0\} \cup \{n\lambda;\ \lambda\in\sigma(A) \}.
    \]
    Non-zero eigenvalues of \( \mathbb{A} \)
        have corresponding block eigenvectors:
    \[
        f_j = \frac{1}{\sqrt{n}} (h_j, \ldots, h_j)\in \mathbb{K}^{mn},\ j=\overline{1,m}.
    \]
    The null-space of \( \mathbb{A} \)
        is formed by the following orthonormal system of vectors:
    \[
        f_{j,k} = \frac{1}{\sqrt{k(k+1)}}
        (
        \underbrace{e_j, \ldots, e_j}_{k\ \text{copies}},
        -ke_j,
        0, \ldots, 0
        ) \in\mathbb{K}^{{mn}{\times}{mn}}
    \]

    The matrix \( \mathbb{A} \) is similar to a block-diagonal matrix
    \[
        \mathcal{A} =
        \left(\begin{array}{c|c}
            \operatorname{diag}(n\lambda_1,\ldots,n\lambda_m) & \mathbf{0} \\ \hline
            \mathbf{0} & \mathbf{0}
        \end{array}\right)\in\mathbb{K}^{{mn}{\times}{mn}}.
    \]
    The similarity transform matrix is
    \[
        U = \operatorname{columns}
        \left(f_1, \ldots, f_m, f_{1,1}, \ldots, f_{1,n{-1}}, \ldots, f_{m,n{-}1}\right).
    \]
\/
}

Throughout this subsection
    we will consider block matrices
    of the size \( {mn}{\times}{mn} \)
    in the form
    \[
    X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} & \cdots & x_{1m} \\
                \vdots & \ddots & \vdots \\
                x_{m1} & \cdots & x_{mm}
            \end{matrix} &
            \begin{matrix}
                x_{1,m+1} \\
                \vdots \\
                x_{m,m+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                x_{m+1,1} &
                \cdots &
                x_{m+1,m}
            \end{matrix} &
            X_{m+1,m+1}
        \end{array}\right),
    \]
where
\( X_{ij}      {=} x_{ij},
 \ X_{m{+}1,j} {=} x_{m{+}1,j},
 \ X_{i,m{+}1} {=} x_{i,m{+}1} \in \mathbb{K} \)
for \( 1 \leq {i,j} \leq m \),
and
\( X_{m{+}1,m{+}1} \) is a block of the size \( {m(n{-}1){\times}m(n-1)} \).

Just like before we are going to investigate
    the spectral behaviour of \( \mathbb{A} \) under perturbations.
We begin with constructing an admissible triple.
A natural choice for \( J \) in this case is
\[
        J X =
        \left(\begin{array}{c|c}
            \begin{matrix}
                x_{11} &  & 0 \\
                 & \ddots &  \\
                0 &  & x_{mm}
            \end{matrix} &
            \begin{matrix}
                0 \\
                \vdots \\
                0
            \end{matrix} \\ \hline
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} &
            X_{m+1,m+1}
        \end{array}\right).
\]

\textbf{Lemma.}
{\it
Suppose \( A \) has a simple spectrum,
    i.e.\ its eigenvalues are pairwise distinct:
    \( \lambda_i\neq\lambda_j \) for all \( 1\leq i{\neq}j \leq m \).

    Then a tuple \( (\mathbb{K}^{{mn}{\times}{mn}}, J, \Gamma) \)
        forms an admissible triple if we define
    \[
        \Gamma X = 
            \frac1n \left(\begin{array}{c|c}
            \begin{matrix}
                0               & \gamma_{12}x_{12} & \cdots & \gamma_{1m}x_{1m} \\
                \gamma_{21}x_{21}  & 0              & \cdots & \gamma_{2m}x_{2m} \\
                \vdots          & \vdots         & \ddots & \vdots & \ \\
                \gamma_{m1}x_{m1}  & \gamma_{m2}x_{m2} & \cdots & 0
            \end{matrix} &
            \begin{matrix}
                \gamma_{1,m+1}x_{1,m+1} \\
                \gamma_{2,m+1}x_{2,m+1} \\
                \vdots \\
                \gamma_{m,m+1}x_{m,m+1}
            \end{matrix} \\ \hline
            \begin{matrix}
                \gamma_{m{+}1,1}x_{m{+}1,1} &
                \gamma_{m{+}1,2}x_{m{+}1,2} &
                \cdots &
                \gamma_{m{+}1,m}x_{m{+}1,m}
            \end{matrix} &
            \mathbf{0}
        \end{array}\right),
    \]
    \[
        \gamma_{ij} = \left\{
            \begin{aligned}
                & \frac{1}{\lambda_i - \lambda_j},\ 1\leq i{\neq}j \leq m{+}1,\\
                & 0,\ i=j
            \end{aligned}
            \right.
    \]
    and we use the convention:
    \[
        \lambda_{m{+}1} = 0.
    \]

    The operator norm of \( \Gamma \) is:
    \[
        \|\Gamma\|_{\mathrm{op}} =
        \frac1n
        \frac{1}{\min_{1\leq i{\neq}j \leq m{+}1}|\lambda_i - \lambda_j|} =
        \]
    \[
        = \frac1n
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq i{\neq}j \leq m }{|\lambda_i - \lambda_j|}},
         \frac{1}{
             \min\limits_{1\leq j \leq m}{|\lambda_j|}}
         \right\}
        \]
\/}

\textbf{Theorem.}
{\it
Suppose \( A \) has simple spectrum and the following inequality holds:
\[
    \left\| \mathbb{B} \right\|_{\mathrm{op}}
        \leq 
        \frac{n}{4}
         \min\left\{
             \min\limits_{1\leq i{\neq}j \leq m }{|\lambda_i - \lambda_j|},
             \min\limits_{1\leq j \leq m}{|\lambda_j|}
         \right\}.
 \]

Then the spectrum of disturbed matrix \( \mathbb{A} - \mathbb{B} \) is
\[
    \sigma\left(\mathbb{A}\right) =
        \left\{
            n\lambda_1 - x_{11}^o, \ldots, n\lambda_m - x_{mm}^o
        \right\}
    \cup \sigma_{m{+}1}.
\]

The eigenvectors
    \( \hat{f}_j,\ \hat{f}_{j,k},\ j{=}\overline{1,m},\ k{=}\overline{1,n{-1}} \)
    of the matrix \( \mathbb{A}{-}\mathbb{B} \),
    the values \( x_{jj}^o,\ j{=}\overline{1,m} \)
    and the set \( \sigma_{m{+}1} \) have the following bounds:
\[
    \lvert x_{jj}^o\rvert,
    \ \max_{\lambda\in\sigma_{m{+}1}} \lvert\lambda\rvert
    \leq 4\|B\|,
\]
\[
    \left\| \hat{f}_j - f_j \right\|_2,
    \ \left\| \hat{f}_{j,k} - f_{j,k}\right\|_2
    \leq
    \frac4n \|B\|
         \max\left\{
         \frac{1}{
             \min\limits_{1\leq l{\neq}p \leq m }{|\lambda_l - \lambda_p|}},
         \frac{1}{
             \min\limits_{1\leq l \leq m}{|\lambda_l|}}
         \right\}
\]
for all \( j{=}\overline{1,m}, k{=}\overline{1,n-1} \).
\/}

\subsection*{Kronecker products}

Now we consider Kronecker products
\[
    A\otimes B =
    \begin{pmatrix}
        a_{11} B & \cdots & a_{1n} B \\
        \vdots   & \ddots & \vdots \\
        a_{n1} B & \cdots & a_{nn} B
    \end{pmatrix}
    \in \mathbb{K}^{{mn}{\times}{mn}}
\]
of squared matrices
\( A={(a_{ij})}\in\mathbb{K}^{n{\times}n},
 \ B={(b_{ij})}\in\mathbb{K}^{m{\times}m} \)
and small-norm perturbations
\[
    A\otimes B - F.
\]

We will also address special
    perturbations of the following form (cf.~\cite{XIANG2005210}):
\[
    (A-\Delta A)\otimes (B - \Delta B).
\]

Note that a "tiled" matrix from the last example
    can be represented as a Kronecker product:
\[
    \mathbb{A} =
    \begin{pmatrix}
    A & \cdots & A\\
    \vdots & \ddots & \vdots \\
    A & \cdots & A\end{pmatrix} =
        J_N\otimes A.
    \]

(This subsection is yet to be completed)

\section*{Conclusion}
The method of similar operators
    is a powerful tool in perturbation analysis.
Developed in context of Banach spaces
    it is just as well applicable
    in finite-dimensional problems.
\section*{References}
\bibliographystyle{iopart-num}
\bibliography{sergey_kozlukov_jpcs}{}
\end{document}
